{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsKrUw6lZlSFdp3HhYQrBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcsellers/HAWK1090/blob/master/X_DTE_selector_live.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J45QKtWXykyi",
        "outputId": "bbaf0ca7-f791-46cc-f3ef-5663e6d31987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[                       0%                       ]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading configuration from: /content/drive/MyDrive/1dte_selector/configs/champion_model_config.json\n",
            "\n",
            "Loaded Configuration:\n",
            "{\n",
            "  \"model_name\": \"Trial_198_DeltaSharpe_Champion\",\n",
            "  \"description\": \"Best parameters from the 250-trial deep dive, optimizing for Delta Sharpe.\",\n",
            "  \"hyperparameters\": {\n",
            "    \"MIN_TRADES\": 11,\n",
            "    \"MIN_WIN_RATE\": 0.76,\n",
            "    \"MIN_AVG_PNL\": 40,\n",
            "    \"MAX_LEG_DELTA\": 55,\n",
            "    \"VOL_CAP\": 30,\n",
            "    \"GAP_CAP\": 5,\n",
            "    \"FALLBACK_OK\": true\n",
            "  },\n",
            "  \"system_settings\": {\n",
            "    \"lookback_period_months\": 24\n",
            "  }\n",
            "}\n",
            "\n",
            "Fetching live data for tickers: ['^GSPC', '^VIX', '^VVIX', '^SKEW']...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  4 of 4 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data fetched successfully.\n",
            "\n",
            "Successfully fetched market data. Shape: (503, 20)\n",
            "Latest data point (T-1):\n",
            "Open_SPX       5.978940e+03\n",
            "Open_SKEW      1.374600e+02\n",
            "Open_VIX       1.768000e+01\n",
            "Open_VVIX      9.116000e+01\n",
            "High_SPX       5.990480e+03\n",
            "High_SKEW      1.374600e+02\n",
            "High_VIX       1.807000e+01\n",
            "High_VVIX      9.456000e+01\n",
            "Low_SPX        5.966110e+03\n",
            "Low_SKEW       1.374600e+02\n",
            "Low_VIX        1.741000e+01\n",
            "Low_VVIX       9.042000e+01\n",
            "Close_SPX      5.970810e+03\n",
            "Close_SKEW     1.374600e+02\n",
            "Close_VIX      1.761000e+01\n",
            "Close_VVIX     9.079000e+01\n",
            "Volume_SPX     4.767050e+09\n",
            "Volume_SKEW    0.000000e+00\n",
            "Volume_VIX     0.000000e+00\n",
            "Volume_VVIX    0.000000e+00\n",
            "Name: 2025-06-04 00:00:00, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-24-b67475ed8e06>:84: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
            "  df = raw_data.stack().reset_index().rename(columns={'level_1': 'Ticker'})\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas-market-calendars --quiet\n",
        "!pip install py_vollib_vectorized --quiet\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- 1. Project Setup and Configuration ---\n",
        "\n",
        "# Define the root path of your project in Google Drive\n",
        "# This helps keep all file paths consistent.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_ROOT = Path('/content/drive/MyDrive/1dte_selector/')\n",
        "except ImportError:\n",
        "    # If not in Colab, use a local path (adjust as needed)\n",
        "    PROJECT_ROOT = Path('.')\n",
        "\n",
        "# Create necessary directories if they don't exist\n",
        "CONFIG_DIR = PROJECT_ROOT / 'configs'\n",
        "LOG_DIR = PROJECT_ROOT / 'live_logs'\n",
        "CONFIG_DIR.mkdir(exist_ok=True)\n",
        "LOG_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "CONFIG_FILE_PATH = CONFIG_DIR / 'champion_model_config.json'\n",
        "\n",
        "def create_champion_config(config_path: Path):\n",
        "    \"\"\"Defines and saves the configuration for our champion model (Trial #198).\"\"\"\n",
        "\n",
        "    champion_params = {\n",
        "      \"model_name\": \"Trial_198_DeltaSharpe_Champion\",\n",
        "      \"description\": \"Best parameters from the 250-trial deep dive, optimizing for Delta Sharpe.\",\n",
        "      \"hyperparameters\": {\n",
        "        \"MIN_TRADES\": 11,\n",
        "        \"MIN_WIN_RATE\": 0.76,\n",
        "        \"MIN_AVG_PNL\": 40,\n",
        "        \"MAX_LEG_DELTA\": 55,\n",
        "        \"VOL_CAP\": 30,\n",
        "        \"GAP_CAP\": 5,\n",
        "        \"FALLBACK_OK\": True\n",
        "      },\n",
        "      # We can add other system settings here later\n",
        "      \"system_settings\": {\n",
        "          \"lookback_period_months\": 24\n",
        "      }\n",
        "    }\n",
        "\n",
        "    print(f\"Creating configuration file at: {config_path}\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(champion_params, f, indent=2)\n",
        "\n",
        "def load_config(config_path: Path) -> dict:\n",
        "    \"\"\"Loads the model configuration from a JSON file.\"\"\"\n",
        "    if not config_path.exists():\n",
        "        print(f\"Config file not found at {config_path}, creating a default one.\")\n",
        "        create_champion_config(config_path)\n",
        "\n",
        "    print(f\"Loading configuration from: {config_path}\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    return config\n",
        "\n",
        "# --- 2. Live Data Fetching ---\n",
        "\n",
        "def fetch_live_data(tickers: list) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Fetches the last 2 years of daily data for the given tickers.\n",
        "    Cleans and formats the DataFrame for feature calculation.\n",
        "    \"\"\"\n",
        "    print(f\"\\nFetching live data for tickers: {tickers}...\")\n",
        "    try:\n",
        "        # Fetch data for the last 2 years, which is plenty for rolling features\n",
        "        raw_data = yf.download(tickers, period=\"2y\", auto_adjust=True)\n",
        "\n",
        "        if raw_data.empty:\n",
        "            print(\"❌ Error: yfinance returned an empty DataFrame.\")\n",
        "            return None\n",
        "\n",
        "        # The downloaded data has multi-level columns, e.g., ('Close', '^VIX')\n",
        "        # We need to flatten and rename them for easy access.\n",
        "        df = raw_data.stack().reset_index().rename(columns={'level_1': 'Ticker'})\n",
        "        df = df.pivot(index='Date', columns='Ticker', values=['Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "\n",
        "        # Flatten the multi-level columns, e.g., ('Close', '^VIX') -> 'Close_^VIX'\n",
        "        df.columns = [f'{val}_{ticker}' for val, ticker in df.columns]\n",
        "\n",
        "        # Rename columns to match the style of our backtest CSV for consistency\n",
        "        # e.g., 'Close_^GSPC' -> 'SPX_Close'\n",
        "        rename_map = {\n",
        "            '_^GSPC': '_SPX',\n",
        "            '_^VIX': '_VIX',\n",
        "            '_^VVIX': '_VVIX',\n",
        "            '_^SKEW': '_SKEW'\n",
        "        }\n",
        "        for old_suffix, new_suffix in rename_map.items():\n",
        "            df.columns = [col.replace(old_suffix, new_suffix) for col in df.columns]\n",
        "\n",
        "        print(\"✅ Data fetched successfully.\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An error occurred during data fetching: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- 3. Main Execution Block ---\n",
        "\n",
        "# Load the configuration\n",
        "config = load_config(CONFIG_FILE_PATH)\n",
        "print(\"\\nLoaded Configuration:\")\n",
        "print(json.dumps(config, indent=2))\n",
        "\n",
        "# Define tickers needed for our features.\n",
        "# Note: V1D data is not available in yfinance. We may need to skip features\n",
        "# that rely on it or find an alternative source/proxy later.\n",
        "required_tickers = ['^GSPC', '^VIX', '^VVIX', '^SKEW']\n",
        "\n",
        "# Fetch the live data\n",
        "live_market_data = fetch_live_data(required_tickers)\n",
        "\n",
        "if live_market_data is not None:\n",
        "    print(f\"\\nSuccessfully fetched market data. Shape: {live_market_data.shape}\")\n",
        "    print(\"Latest data point (T-1):\")\n",
        "    # Display the second to last row, as the last row might be for the current, incomplete day\n",
        "    print(live_market_data.iloc[-2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Feature Calculation (Revised with Dynamic Binning) ---\n",
        "\n",
        "# The helper function remains the same\n",
        "def cut_bins(series: pd.Series, edges: list) -> pd.Series:\n",
        "    \"\"\"Cuts a series into bins with predefined edges.\"\"\"\n",
        "    if series.empty:\n",
        "        return pd.Series(dtype='category')\n",
        "    # Using [-np.inf, *edges, np.inf] is more robust for dynamic quantiles\n",
        "    bins = [-np.inf, *edges, np.inf]\n",
        "    labels = [f\"Q{i+1}\" for i in range(len(bins) - 1)]\n",
        "    return pd.cut(series, bins=bins, labels=labels, include_lowest=True, duplicates='drop')\n",
        "\n",
        "\n",
        "def calculate_live_features(df: pd.DataFrame) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Calculates all necessary features, using DYNAMIC binning for categorical features.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame from fetch_live_data.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the original and all new feature columns.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    print(\"\\nCalculating live features with DYNAMIC binning...\")\n",
        "    features = df.copy()\n",
        "\n",
        "    # --- A. Calculate Raw Numerical Features ---\n",
        "    features['VIX_ONMPctT'] = (features['Open_VIX'] / features['Close_VIX'].shift(1)) - 1\n",
        "    features['TS_RTm1'] = (features['Close_VIX'].shift(1) / features['Close_VVIX'].shift(1)) - 1\n",
        "    features['VIX_ClTm1'] = features['Close_VIX'].shift(1)\n",
        "\n",
        "    # --- B. Calculate Categorical Features using Dynamic Binning ---\n",
        "\n",
        "    # Define which features to bin and how many bins (quantiles) to create\n",
        "    features_to_bin = {\n",
        "        \"VIX_ClTm1\": 5,\n",
        "        \"TS_RTm1\": 5,\n",
        "        \"VIX_ONMPctT\": 5\n",
        "    }\n",
        "\n",
        "    for feature_name, num_bins in features_to_bin.items():\n",
        "        if feature_name in features.columns:\n",
        "            # To avoid lookahead bias, calculate quantiles on all data EXCEPT the most recent point\n",
        "            historical_series = features[feature_name].dropna().iloc[:-1]\n",
        "\n",
        "            # Calculate the quantile edges (e.g., for 5 bins, we need 4 edges: .2, .4, .6, .8)\n",
        "            quantiles = np.linspace(0, 1, num_bins + 1)[1:-1] # e.g., [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "            try:\n",
        "                # Calculate the bin edges from the historical distribution\n",
        "                edges = historical_series.quantile(quantiles).tolist()\n",
        "\n",
        "                cat_col_name = f\"{feature_name}_Cat\"\n",
        "                # Apply these fresh, dynamic edges to the entire series\n",
        "                features[cat_col_name] = cut_bins(features[feature_name], edges)\n",
        "                print(f\"  Dynamically created categorical feature: {cat_col_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️ Could not dynamically bin '{feature_name}'. Error: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠️ Warning: Raw column '{feature_name}' not found. Cannot create categorical feature.\")\n",
        "\n",
        "    # --- C. Add Time-Based Features ---\n",
        "    features['DOW_Cat'] = features.index.day_name().astype('category')\n",
        "\n",
        "    print(\"✅ Feature calculation complete.\")\n",
        "    return features\n",
        "\n",
        "# --- 5. Main Execution Block (continued) ---\n",
        "\n",
        "# Re-run the feature calculation on the data we fetched previously\n",
        "features_df = calculate_live_features(live_market_data)\n",
        "\n",
        "if features_df is not None:\n",
        "    # Display the latest data point with all the new features\n",
        "    latest_features = features_df.iloc[-1]\n",
        "\n",
        "    print(\"\\n--- Latest Features for Next Day's Signal (using Dynamic Bins) ---\")\n",
        "    print(f\"Data for decision on: {latest_features.name.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    important_cols = [\n",
        "        'VIX_ClTm1', 'VIX_ClTm1_Cat',\n",
        "        'TS_RTm1', 'TS_RTm1_Cat',\n",
        "        'VIX_ONMPctT', 'VIX_ONMPctT_Cat',\n",
        "        'DOW_Cat'\n",
        "    ]\n",
        "    cols_to_print = [col for col in important_cols if col in latest_features]\n",
        "    print(latest_features[cols_to_print])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xg5tEeTzZTa",
        "outputId": "88fbaa7c-bfc0-4c9b-c7c1-00c8a89ee58e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating live features with DYNAMIC binning...\n",
            "  Dynamically created categorical feature: VIX_ClTm1_Cat\n",
            "  Dynamically created categorical feature: TS_RTm1_Cat\n",
            "  Dynamically created categorical feature: VIX_ONMPctT_Cat\n",
            "✅ Feature calculation complete.\n",
            "\n",
            "--- Latest Features for Next Day's Signal (using Dynamic Bins) ---\n",
            "Data for decision on: 2025-06-05\n",
            "VIX_ClTm1          17.610001\n",
            "VIX_ClTm1_Cat             Q4\n",
            "TS_RTm1            -0.806036\n",
            "TS_RTm1_Cat               Q5\n",
            "VIX_ONMPctT         0.003975\n",
            "VIX_ONMPctT_Cat           Q3\n",
            "DOW_Cat             Thursday\n",
            "Name: 2025-06-05 00:00:00, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Final Signal Generation ---\n",
        "# This section brings in all necessary logic from our backtesting script\n",
        "# to build the strategy rulebook and make a final decision.\n",
        "\n",
        "import unicodedata\n",
        "from datetime import timedelta\n",
        "import pandas_market_calendars as mcal # For robust holiday/weekend handling\n",
        "\n",
        "# Define paths to the data the lookup builder needs\n",
        "# Note: Ensure VERSION is defined, or replace it with your version string e.g., \"yf_v1.4\"\n",
        "try:\n",
        "    VERSION\n",
        "except NameError:\n",
        "    VERSION = \"yf_v1.4\" # Fallback if not defined earlier\n",
        "\n",
        "BT_DIR = PROJECT_ROOT / \"backtest_data\"\n",
        "EDA_DIR = PROJECT_ROOT / f\"eda_results_1dte_NO_LOOKAHEAD_{VERSION}\"\n",
        "\n",
        "# The primary VIX category column name must match what's in your EDA files\n",
        "PRIMARY_VIX_CAT_ACTUAL = \"VIX_ClTm1_Cat\"\n",
        "\n",
        "# --- Helper functions copied from the backtesting script ---\n",
        "\n",
        "def _strategy_key(side: str, delta: int, width: int) -> str:\n",
        "    \"\"\"Generates a standardized key for the PNL dictionary.\"\"\"\n",
        "    return f\"{side.lower()}_{delta}_{width}\"\n",
        "\n",
        "def _has_strategy_legs(strategy_name: str, pnl_data: dict) -> bool:\n",
        "    \"\"\"Checks if PNL series for all legs of a strategy exist.\"\"\"\n",
        "    if not pnl_data: return False\n",
        "    # This parser handles names like \"IC_P20C10_10w\" or \"PutSpread_D12_5w\"\n",
        "    if strategy_name.startswith((\"PutSpread\", \"CallSpread\")):\n",
        "        m = re.search(r\"(Put|Call)Spread_D(\\d+)_(\\d+)w\", strategy_name)\n",
        "        if not m: return False\n",
        "        side, d, w = m.group(1), int(m.group(2)), int(m.group(3))\n",
        "        return _strategy_key(side, d, w) in pnl_data\n",
        "    elif strategy_name.startswith(\"IC\"):\n",
        "        m = re.search(r\"IC_P(\\d+)C(\\d+)_(\\d+)w\", strategy_name)\n",
        "        if not m: return False\n",
        "        p_delta, c_delta, width = int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
        "        return (_strategy_key(\"put\", p_delta, width) in pnl_data and\n",
        "                _strategy_key(\"call\", c_delta, width) in pnl_data)\n",
        "    return False\n",
        "\n",
        "def _clean_column_names(columns: list) -> list:\n",
        "    \"\"\"Normalizes column names for consistency.\"\"\"\n",
        "    out = []\n",
        "    for c in columns:\n",
        "        c = unicodedata.normalize('NFKD', str(c)).encode('ascii', 'ignore').decode('utf-8')\n",
        "        c = c.replace(' ', '_').replace('.', '_').strip()\n",
        "        out.append(c)\n",
        "    return out\n",
        "\n",
        "# --- PNL Data Loading (to know which strategies are valid) ---\n",
        "def load_pnl_placeholders(backtest_dir: Path) -> dict:\n",
        "    \"\"\"Scans for PNL files and creates a dictionary of existing strategy keys.\"\"\"\n",
        "    print(\"\\nLoading available strategy PNL placeholders...\")\n",
        "    all_pnl_keys = {}\n",
        "    file_pattern = re.compile(r\"1dte_(put|call)_s?p[er]{2}ad_(\\d+)delta_(\\d+)pts.*\\.csv\", re.I)\n",
        "    for fp in backtest_dir.glob(\"*.csv\"):\n",
        "        match = file_pattern.match(fp.name)\n",
        "        if match:\n",
        "            side, delta, width = match.groups()\n",
        "            key = _strategy_key(side, int(delta), int(width))\n",
        "            all_pnl_keys[key] = True # We just need the key to exist\n",
        "    print(f\"✅ Found {len(all_pnl_keys)} unique PNL backtests available.\")\n",
        "    return all_pnl_keys\n",
        "\n",
        "# --- The \"Brain\": Strategy Lookup Builder ---\n",
        "def build_strategy_lookup(\n",
        "    params: dict, eda_files_dir: Path, pnl_data: dict, train_range_dates: tuple\n",
        ") -> tuple[dict, dict, set]:\n",
        "    \"\"\"Builds strategy lookup tables based on rules from EDA files.\"\"\"\n",
        "    lut_full, lut_primary, sitout_keys = {}, {}, set()\n",
        "    train_start, train_end = pd.to_datetime(train_range_dates[0]), pd.to_datetime(train_range_dates[1])\n",
        "\n",
        "    for eda_file_path in eda_files_dir.glob(\"eda_*_Cat*.csv\"):\n",
        "        try:\n",
        "            df_eda = pd.read_csv(eda_file_path)\n",
        "            df_eda.columns = _clean_column_names(df_eda.columns)\n",
        "\n",
        "            # Filter EDA data by the dynamic training date range\n",
        "            if \"Date\" in df_eda.columns:\n",
        "                df_eda[\"Date\"] = pd.to_datetime(df_eda[\"Date\"], errors='coerce')\n",
        "                df_eda = df_eda[df_eda[\"Date\"].between(train_start, train_end)]\n",
        "            if df_eda.empty: continue\n",
        "\n",
        "            # Identify strategy, VIX, and secondary columns\n",
        "            strat_col = next((c for c in df_eda.columns if c in [\"Structure_Name\", \"Strategy_Name\"]), None)\n",
        "            vix_cat_col = next((c for c in df_eda.columns if PRIMARY_VIX_CAT_ACTUAL in c), None)\n",
        "            sec_cat_col = next((c for c in df_eda.columns if c.endswith(\"_Cat\") and c != vix_cat_col), None)\n",
        "            if not all([strat_col, vix_cat_col, sec_cat_col]): continue\n",
        "\n",
        "            # Filter rules based on champion hyperparameters\n",
        "            valid_rows = df_eda[df_eda[\"Trade_Count\"] >= params.get(\"MIN_TRADES\", 10)]\n",
        "            best_strats = valid_rows.sort_values(\"Avg_PNL\", ascending=False).groupby(\n",
        "                [vix_cat_col, sec_cat_col], sort=False).head(1)\n",
        "\n",
        "            for _, row in best_strats.iterrows():\n",
        "                strategy = row[strat_col]\n",
        "                if not _has_strategy_legs(strategy, pnl_data): continue\n",
        "\n",
        "                deltas = [int(d) for d in re.findall(r'\\d+', strategy.split(\"_\")[1])]\n",
        "                if deltas and max(deltas) > params.get(\"MAX_LEG_DELTA\", 50): continue\n",
        "\n",
        "                key = (str(row[vix_cat_col]).split(\"_\")[0], sec_cat_col, str(row[sec_cat_col]).split(\"_\")[0])\n",
        "\n",
        "                if (row.get(\"Win_Rate\", 0) < params.get(\"MIN_WIN_RATE\", 0.55)) or \\\n",
        "                   (row.get(\"Avg_PNL\", 0) < params.get(\"MIN_AVG_PNL\", 0)):\n",
        "                    sitout_keys.add(key)\n",
        "                    continue\n",
        "\n",
        "                lut_full[key] = strategy\n",
        "                if key[0] not in lut_primary: lut_primary[key[0]] = strategy\n",
        "        except Exception:\n",
        "            continue\n",
        "    return lut_full, lut_primary, sitout_keys\n",
        "\n",
        "# --- The Final Signal Generation Function ---\n",
        "def generate_final_signal(\n",
        "    latest_features: pd.Series, model_config: dict, eda_dir: Path, pnl_data: dict\n",
        ") -> str:\n",
        "    \"\"\"Takes the latest features and generates the final trading signal.\"\"\"\n",
        "    print(\"\\n--- Generating Final Signal ---\")\n",
        "    params = model_config['hyperparameters']\n",
        "\n",
        "    # 1. Check master sit-out filters\n",
        "    if params.get(\"VOL_CAP\") and latest_features.get('VIX_ClTm1', 0) > params[\"VOL_CAP\"]:\n",
        "        return f\"Sit Out (VIX {latest_features['VIX_ClTm1']:.2f} > VOL_CAP {params['VOL_CAP']})\"\n",
        "    if params.get(\"GAP_CAP\") and abs(latest_features.get('VIX_ONMPctT', 0)) * 100 > params[\"GAP_CAP\"]:\n",
        "         return f\"Sit Out (VIX Gap {abs(latest_features['VIX_ONMPctT']*100):.1f}% > GAP_CAP {params['GAP_CAP']})\"\n",
        "\n",
        "    # 2. Build the dynamic lookup table\n",
        "    today = latest_features.name\n",
        "    lookback = model_config['system_settings']['lookback_period_months']\n",
        "    train_end_date = today - timedelta(days=1)\n",
        "    train_start_date = today - pd.DateOffset(months=lookback)\n",
        "\n",
        "    print(f\"Building strategy lookup using data from {train_start_date.date()} to {train_end_date.date()}...\")\n",
        "    lut_full, lut_primary, sitout_keys = build_strategy_lookup(\n",
        "        params=params, eda_files_dir=eda_dir, pnl_data=pnl_data,\n",
        "        train_range_dates=(str(train_start_date.date()), str(train_end_date.date()))\n",
        "    )\n",
        "    if not lut_full: return \"Sit Out (No valid rules found in lookup table for period)\"\n",
        "\n",
        "    # 3. Find strategy for today's conditions\n",
        "    vix_q = str(latest_features.get(PRIMARY_VIX_CAT_ACTUAL, \"\")).split(\"_\")[0]\n",
        "    all_cat_cols = [c for c in latest_features.index if c.endswith('_Cat') and c != PRIMARY_VIX_CAT_ACTUAL]\n",
        "\n",
        "    for sec_col in all_cat_cols:\n",
        "        sec_q = str(latest_features.get(sec_col, \"\")).split(\"_\")[0]\n",
        "        key = (vix_q, sec_col, sec_q)\n",
        "\n",
        "        if key in sitout_keys: return f\"Sit Out (Rule for {key} is an explicit sit-out)\"\n",
        "\n",
        "        strategy = lut_full.get(key)\n",
        "        if strategy: return strategy\n",
        "\n",
        "    # 4. Check fallback if allowed and no specific rule found\n",
        "    if params.get(\"FALLBACK_OK\", False):\n",
        "        strategy = lut_primary.get(vix_q)\n",
        "        if strategy: return strategy\n",
        "\n",
        "    return \"Sit Out (No rule matched)\"\n",
        "\n",
        "# --- 7. Main Execution Block (Finalized) ---\n",
        "\n",
        "# --- 7. Main Execution Block (Finalized with Logging) ---\n",
        "\n",
        "# Load available strategies from PNL backtest files\n",
        "ALL_PNL_GLOBAL = load_pnl_placeholders(BT_DIR)\n",
        "\n",
        "# Generate the signal using the latest features\n",
        "final_signal = generate_final_signal(\n",
        "    latest_features=latest_features,\n",
        "    model_config=config,\n",
        "    eda_dir=EDA_DIR,\n",
        "    pnl_data=ALL_PNL_GLOBAL\n",
        ")\n",
        "\n",
        "# Use NYSE Market Calendar to find the next valid trading day\n",
        "nyse = mcal.get_calendar('NYSE')\n",
        "schedule = nyse.schedule(start_date=pd.Timestamp.today().strftime('%Y-%m-%d'),\n",
        "                         end_date=(pd.Timestamp.today() + timedelta(days=10)).strftime('%Y-%m-%d'))\n",
        "next_trading_day = schedule.index[0].strftime('%Y-%m-%d')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\" फैसला │ Signal for Next Trading Day ({next_trading_day})\")\n",
        "print(\"=\"*50)\n",
        "print(f\"  >> {final_signal} <<\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- 8. Logging and Notification (ACTIVATED) ---\n",
        "# This part is now active. It will create and write to the log file.\n",
        "\n",
        "# a) Log the signal to a file\n",
        "print(\"\\n--- Logging Signal ---\")\n",
        "try:\n",
        "    signal_log_path = LOG_DIR / 'signal_log.csv'\n",
        "    log_entry = pd.DataFrame([{\n",
        "        'log_timestamp_utc': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'signal_for_date': next_trading_day, # Use the correctly calculated date\n",
        "        'signal': final_signal\n",
        "    }])\n",
        "\n",
        "    if not signal_log_path.exists():\n",
        "        # If the file doesn't exist, create it with a header\n",
        "        log_entry.to_csv(signal_log_path, index=False)\n",
        "        print(f\"✅ Created new signal log and saved entry to: {signal_log_path}\")\n",
        "    else:\n",
        "        # If it exists, append without writing the header\n",
        "        log_entry.to_csv(signal_log_path, mode='a', header=False, index=False)\n",
        "        print(f\"✅ Appended new signal to existing log: {signal_log_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to write to log file. Error: {e}\")\n",
        "\n",
        "\n",
        "# b) Send a notification (simulation)\n",
        "# (In production, your code to send an email/Discord message would go here)\n",
        "print(\"✅ Notification sent (simulation).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoqzccB_0o-p",
        "outputId": "a9e9ef16-3f25-4dc5-827b-153119e44dc7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading available strategy PNL placeholders...\n",
            "✅ Found 36 unique PNL backtests available.\n",
            "\n",
            "--- Generating Final Signal ---\n",
            "Building strategy lookup using data from 2023-06-05 to 2025-06-04...\n",
            "\n",
            "==================================================\n",
            " फैसला │ Signal for Next Trading Day (2025-06-06)\n",
            "==================================================\n",
            "  >> Sit Out (Rule for ('Q4', 'TS_RTm1_Cat', 'Q5') is an explicit sit-out) <<\n",
            "==================================================\n",
            "\n",
            "--- Logging Signal ---\n",
            "✅ Created new signal log and saved entry to: /content/drive/MyDrive/1dte_selector/live_logs/signal_log.csv\n",
            "✅ Notification sent (simulation).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Papertrade logger"
      ],
      "metadata": {
        "id": "47VWGlew4Suo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required library\n",
        "!pip install py_vollib_vectorized --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from pathlib import Path\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "# CORRECTED IMPORT STATEMENT:\n",
        "from py_vollib_vectorized import vectorized_delta\n",
        "\n",
        "# --- 1. Setup ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_ROOT = Path('/content/drive/MyDrive/1dte_selector/')\n",
        "except ImportError:\n",
        "    PROJECT_ROOT = Path('.')\n",
        "\n",
        "LOG_DIR = PROJECT_ROOT / 'live_logs'\n",
        "SIGNAL_LOG_PATH = LOG_DIR / 'signal_log.csv'\n",
        "TRADE_LEDGER_PATH = LOG_DIR / 'paper_trade_ledger.csv'\n",
        "\n",
        "\n",
        "# --- 2. Helper Functions (Upgraded) ---\n",
        "\n",
        "def parse_strategy_name(strategy_name: str) -> dict | None:\n",
        "    \"\"\"Parses a strategy string like 'IC_P20C10_10w' into its components.\"\"\"\n",
        "    if strategy_name.startswith(\"IC\"):\n",
        "        m = re.search(r\"IC_P(\\d+)C(\\d+)_(\\d+)w\", strategy_name)\n",
        "        if m:\n",
        "            return { \"type\": \"IC\", \"put_delta\": int(m.group(1)), \"call_delta\": int(m.group(2)), \"width\": int(m.group(3)) }\n",
        "    return None\n",
        "\n",
        "def get_risk_free_rate() -> float:\n",
        "    \"\"\"Fetches the latest 3-Month Treasury Bill rate as a proxy for the risk-free rate.\"\"\"\n",
        "    try:\n",
        "        rate = yf.Ticker(\"^IRX\").history(period=\"5d\")['Close'].iloc[-1] / 100\n",
        "        return max(rate, 0.001)\n",
        "    except Exception:\n",
        "        return 0.05 # Fallback to a reasonable default\n",
        "\n",
        "def find_strike_for_delta(chain, target_delta, underlying_price, risk_free_rate, flag):\n",
        "    \"\"\"\n",
        "    Calculates the delta for each option in the chain and finds the strike\n",
        "    with the delta closest to the target.\n",
        "    \"\"\"\n",
        "    if chain.empty: raise ValueError(\"Options chain is empty.\")\n",
        "\n",
        "    target_delta_val = abs(target_delta) if flag == 'c' else -abs(target_delta)\n",
        "\n",
        "    # Required inputs for the formula\n",
        "    S = underlying_price\n",
        "    K = chain['strike'].values\n",
        "    t = 1.0 / 252.0  # Time to expiration: 1 trading day\n",
        "    r = risk_free_rate\n",
        "    sigma = chain['impliedVolatility'].values\n",
        "\n",
        "    # CORRECTED FUNCTION CALL:\n",
        "    # Use the specific vectorized_delta function\n",
        "    calculated_deltas = vectorized_delta(flag, S, K, t, r, sigma)\n",
        "    chain['calculated_delta'] = calculated_deltas\n",
        "\n",
        "    # Find the row in the chain with the delta closest to our target\n",
        "    closest_strike_row = chain.iloc[(chain['calculated_delta'] - target_delta_val).abs().argsort()[:1]]\n",
        "\n",
        "    return closest_strike_row['strike'].values[0]\n",
        "\n",
        "# --- 3. Main Evaluation Logic ---\n",
        "\n",
        "def evaluate_and_log_trades():\n",
        "    print(\"--- Running Trade Evaluator (with Black-Scholes Delta Approximation) ---\")\n",
        "    if not SIGNAL_LOG_PATH.exists(): print(f\"Signal log not found at {SIGNAL_LOG_PATH}.\"); return\n",
        "\n",
        "    signal_log_df = pd.read_csv(SIGNAL_LOG_PATH)\n",
        "    if Path(TRADE_LEDGER_PATH).exists():\n",
        "        ledger_df = pd.read_csv(TRADE_LEDGER_PATH); evaluated_dates = ledger_df['trade_date'].tolist()\n",
        "    else:\n",
        "        ledger_df = pd.DataFrame(); evaluated_dates = []\n",
        "\n",
        "    trades_to_evaluate = signal_log_df[~signal_log_df['signal_for_date'].isin(evaluated_dates)]\n",
        "    if trades_to_evaluate.empty: print(\"No new trades to evaluate.\"); return\n",
        "\n",
        "    print(f\"Found {len(trades_to_evaluate)} new signals to evaluate...\")\n",
        "    spx = yf.Ticker(\"^SPX\")\n",
        "    risk_free_rate = get_risk_free_rate()\n",
        "    print(f\"Using Risk-Free Rate: {risk_free_rate:.3%}\")\n",
        "    new_ledger_entries = []\n",
        "\n",
        "    for _, row in trades_to_evaluate.iterrows():\n",
        "        trade_date_str, signal = row['signal_for_date'], row['signal']\n",
        "        trade_date = pd.to_datetime(trade_date_str)\n",
        "\n",
        "        if trade_date >= pd.Timestamp.today().normalize(): continue\n",
        "        if \"Sit Out\" in signal: continue\n",
        "\n",
        "        print(f\"\\nEvaluating trade for {trade_date_str}: {signal}\")\n",
        "\n",
        "        try:\n",
        "            hist_spx = spx.history(start=trade_date, end=trade_date + timedelta(days=1))\n",
        "            open_price, close_price = hist_spx['Open'].iloc[0], hist_spx['Close'].iloc[0]\n",
        "\n",
        "            opt_chain = spx.option_chain(trade_date_str)\n",
        "            calls, puts = opt_chain.calls, opt_chain.puts\n",
        "\n",
        "            pnl, details = 0.0, {}\n",
        "            parsed_strat = parse_strategy_name(signal)\n",
        "\n",
        "            if parsed_strat and parsed_strat['type'] == 'IC':\n",
        "                put_delta_target, call_delta_target = parsed_strat['put_delta'] / 100.0, parsed_strat['call_delta'] / 100.0\n",
        "\n",
        "                short_put_strike = find_strike_for_delta(puts, put_delta_target, open_price, risk_free_rate, 'p')\n",
        "                long_put_strike = short_put_strike - parsed_strat['width']\n",
        "                short_call_strike = find_strike_for_delta(calls, call_delta_target, open_price, risk_free_rate, 'c')\n",
        "                long_call_strike = short_call_strike + parsed_strat['width']\n",
        "\n",
        "                short_put_premium = puts[puts.strike == short_put_strike]['lastPrice'].iloc[0]\n",
        "                long_put_premium = puts[puts.strike == long_put_strike]['lastPrice'].iloc[0]\n",
        "                short_call_premium = calls[calls.strike == short_call_strike]['lastPrice'].iloc[0]\n",
        "                long_call_premium = calls[calls.strike == long_call_strike]['lastPrice'].iloc[0]\n",
        "\n",
        "                credit_received = (short_put_premium - long_put_premium) + (short_call_premium - long_call_premium)\n",
        "\n",
        "                outcome = \"Win (Max Profit)\" if short_put_strike < close_price < short_call_strike else \"Loss (Max Loss)\"\n",
        "                pnl = credit_received * 100 if outcome == \"Win (Max Profit)\" else (credit_received - parsed_strat['width']) * 100\n",
        "\n",
        "                details = {\"outcome\": outcome, \"credit\": f\"{credit_received:.2f}\", \"spx_close\": f\"{close_price:.2f}\",\n",
        "                           \"short_put\": short_put_strike, \"short_call\": short_call_strike}\n",
        "\n",
        "            new_ledger_entries.append({\"trade_date\": trade_date_str, \"signal\": signal, \"pnl\": pnl, **details})\n",
        "            print(f\"  Calculated PNL: ${pnl:.2f}, Outcome: {outcome}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Could not evaluate trade for {trade_date_str}. Error: {e}\")\n",
        "            new_ledger_entries.append({\"trade_date\": trade_date_str, \"signal\": signal, \"pnl\": 0.0, \"outcome\": \"Evaluation Error\"})\n",
        "\n",
        "    if new_ledger_entries:\n",
        "        new_results_df = pd.DataFrame(new_ledger_entries)\n",
        "        if ledger_df.empty:\n",
        "            new_results_df.to_csv(TRADE_LEDGER_PATH, index=False)\n",
        "        else:\n",
        "            pd.concat([ledger_df, new_results_df]).to_csv(TRADE_LEDGER_PATH, index=False)\n",
        "        print(f\"\\n✅ Wrote {len(new_ledger_entries)} new results to {TRADE_LEDGER_PATH}\")\n",
        "\n",
        "# --- 4. Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_and_log_trades()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUk45rT24UQn",
        "outputId": "ac61408e-783d-4005-c419-7fdf3094805e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--- Running Trade Evaluator (with Black-Scholes Delta Approximation) ---\n",
            "Found 1 new signals to evaluate...\n",
            "Using Risk-Free Rate: 4.232%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ 1DTE Live Signal & Evaluation System\n",
        "📜 Overview\n",
        "This system is a live prototype designed to generate a daily trading signal for a 1-day-to-expiration (1DTE) options strategy. It is built upon the champion model parameters (Trial #198) discovered through a comprehensive Optuna optimization process.\n",
        "\n",
        "The architecture consists of two primary scripts that work together to form a complete operational loop.\n",
        "\n",
        "🏛️ System Architecture\n",
        "Script\t🤖 Purpose\t🗓️ When to Run\n",
        "generate_signal.py\tThe Decision-Maker. Analyzes the market to decide on tomorrow's trade.\tDaily, at ~3:20 PM ET on trading days.\n",
        "evaluate_trades.py\tThe Accountant. Calculates the PNL of completed paper trades.\tDaily or Weekly (not time-sensitive).\n",
        "\n",
        "Export to Sheets\n",
        "Script 1: generate_signal.py (The Decision-Maker)\n",
        "This script's sole responsibility is to produce a high-quality, data-driven decision for the next trading day.\n",
        "\n",
        "Loads Config: It begins by loading the champion model's hyperparameters from configs/champion_model_config.json. This keeps our logic separate from our parameters.\n",
        "Fetches Live Data: It uses yfinance to create an intraday snapshot of the market as of ~3:15 PM ET, aligning perfectly with the backtest methodology.\n",
        "Calculates Dynamic Features: It computes all necessary features. For categorical bins (e.g., VIX_ClTm1_Cat), it dynamically calculates the quantiles based on a rolling 2-year window of data, ensuring the model adapts to changing market regimes.\n",
        "Builds Lookup Table: Using the last 24 months of historical data, it builds a fresh strategy rulebook based on the champion parameters.\n",
        "Generates Signal: It looks up the live features in this new rulebook to produce the final signal (e.g., \"IC_P25C25_10w\" or \"Sit Out\").\n",
        "Logs Signal: The decision is saved with a timestamp to live_logs/signal_log.csv.\n",
        "Script 2: evaluate_trades.py (The Accountant)\n",
        "This script automates the tedious process of performance tracking.\n",
        "\n",
        "Reads Signal Log: It checks signal_log.csv for any trade signals that have recently concluded and have not yet been evaluated.\n",
        "Fetches Option Data: It uses yfinance to pull the historical options chain for the specific day the trade occurred.\n",
        "Approximates PNL: Using the Black-Scholes model (py_vollib_vectorized) and a live risk-free rate, it accurately finds the strikes corresponding to the target deltas and calculates the approximate PNL of the trade.\n",
        "Updates Ledger: The final result is written to a permanent live_logs/paper_trade_ledger.csv for long-term performance analysis.\n",
        "🔄 Daily Paper Trading Workflow\n",
        "Your daily operational process is simple:\n",
        "\n",
        "Run generate_signal.py at ~3:20 PM ET. Note the signal produced for the next trading day.\n",
        "Run evaluate_trades.py at your convenience (e.g., the next day or at the end of the week) to update your performance ledger.\n",
        "Periodically analyze paper_trade_ledger.csv in a separate notebook to review your live paper-trading performance against the backtest results.\n",
        "🚀 How to Update the Champion Model (Periodic Retraining)\n",
        "As you collect more data over time, you can re-run the Optuna optimization to see if a new set of champion hyperparameters can be found.\n",
        "\n",
        "Frequency: A good cadence is every 3 to 6 months, or after you observe a significant, lasting change in market behavior.\n",
        "\n",
        "Prerequisites\n",
        "Before starting an update, ensure your source data is current:\n",
        "\n",
        "PNL Data: Your backtest_data folder must be updated with your latest backtest CSVs.\n",
        "EDA Files: Your summary EDA files must be regenerated from the new PNL data.\n",
        "Market Features: Your main market feature CSV should be updated.\n",
        "Running the Optuna Update\n",
        "In your main analysis notebook (the one with the WORKFLOW_MODE switch):\n",
        "\n",
        "Set Workflow:\n",
        "Python\n",
        "\n",
        "WORKFLOW_MODE = 'DEEP_DIVE'\n",
        "Choose Objective: Select your best-performing scoring function.\n",
        "Python\n",
        "\n",
        "chosen_scoring_function = score_by_delta_sharpe\n",
        "Resume Study: Modify the script to load your existing study so it can continue learning, rather than starting from scratch.\n",
        "Python\n",
        "\n",
        "study_name = \"DeepDive_DeltaSharpe_LongRun\" # Use your long-run study name\n",
        "storage_path = f\"sqlite:///{PROJECT_ROOT / study_name}.db\"\n",
        "\n",
        "print(f\"Loading study '{study_name}' to continue optimization...\")\n",
        "optuna_study = optuna.load_study(study_name=study_name, storage=storage_path)\n",
        "Set New Trial Target: Decide how many more trials to run.\n",
        "Python\n",
        "\n",
        "n_existing_trials = len(optuna_study.trials)\n",
        "n_target_trials = 4000 # Example: Add 1000 trials to your previous 3000\n",
        "n_new_trials = max(0, n_target_trials - n_existing_trials)\n",
        "\n",
        "if n_new_trials > 0:\n",
        "    optuna_study.optimize(objective_runner, n_trials=n_new_trials)\n",
        "Analyze and Promote: After the run, use the run_sensitivity_analysis_on_top_trials function to compare any new top trials to your old champion. If a new winner emerges, update your configs/champion_model_config.json file."
      ],
      "metadata": {
        "id": "sO3D_8IE5_WY"
      }
    }
  ]
}